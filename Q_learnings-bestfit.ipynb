{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2989f610-84d5-4223-90fe-2676c4f1c9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\shraddha\\.anaconda\\anaconda-navigator\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shraddha\\.anaconda\\anaconda-navigator\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "##import libraries\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"`np.bool8` is a deprecated alias for `np.bool_`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60eed722-6379-4dda-a7db-f4b8fea4f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b856d4ff-8e64-460f-ab70-adfe677ed704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()##only [0] gives the state so we assign Q(s,a) the s here as env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644ce593-f7de-4cbb-aa61-c97b56e898d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "####define all functions-gen_episode,take_action,q_learn,monte_carlo and update as per\n",
    "q_table_ql=defaultdict(lambda:np.zeros(6))\n",
    "q_table_mc=defaultdict(lambda:np.zeros(6))###becase theres 500 possible states a key of that state will be created as when encountered\n",
    "#q table has state as the key, q_table[state] gives a list with index as action value and its value i.e q_table[state][action=index]=q(s,a) value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf81099-b4b0-4a36-b8b1-17b4b9d5b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state,q_table):\n",
    "    if random.random()<epsilon:###greedy algo to take action\n",
    "        action=env.action_space.sample()\n",
    "    else:\n",
    "        action=np.argmax(q_table[state])###since index is the action\n",
    "    return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af927ac5-731b-4c6a-95ba-7f7f246eb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_episode_ql(q_table):\n",
    "    episode_reward=0\n",
    "    done=False\n",
    "    state=env.reset()[0]\n",
    "    while not done:###timestep updation required for q-learning\n",
    "        action=take_action(state,q_table)\n",
    "        next_state,reward,done,_,_=env.step(action)\n",
    "        q_table[state][action]+=alpha*(reward+max(q_table[next_state])*gamma-q_table[state][action])##update for q_learning\n",
    "        state=next_state\n",
    "        episode_reward+=reward\n",
    "    #epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "    return episode_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a210a0-a9f8-4d18-831f-3d9b012cf489",
   "metadata": {},
   "outputs": [],
   "source": [
    "##def update_monte(q_table):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd169e-c178-4aab-8576-f3de3aeda3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q Learning: 100%|██████████| 5000/5000 [00:30<00:00, 162.43it/s]\n",
      "Q Learning:  54%|█████▍    | 2712/5000 [00:24<00:09, 232.36it/s]"
     ]
    }
   ],
   "source": [
    "###function to get all\n",
    "from tqdm import tqdm\n",
    "alpha_values = [0.01, 0.05, 0.1]\n",
    "gamma_values = [0.9, 0.99]\n",
    "epsilon_values = [0.1]\n",
    "results=[]\n",
    "for alpha in alpha_values:\n",
    "    for gamma in gamma_values:\n",
    "        for epsilon in epsilon_values:\n",
    "            \n",
    "            q_table_ql=defaultdict(lambda:np.zeros(6))\n",
    "            #q_table_mc=defaultdict(lambda:np.zeros(6))\n",
    "            #alpha = 0.1  # Learning rate\n",
    "            #gamma = 0.99  # Discount factor\n",
    "            #epsilon = 0.1  # Exploration rate\n",
    "            epsilon_decay = 0.99\n",
    "            min_epsilon = 0.01\n",
    "            episode_data=[]\n",
    "            #episodes = 1000\n",
    "            done=False\n",
    "            x_episodes=[]\n",
    "            reward_after_each_episode_ql=[]\n",
    "            reward_after_each_episode_mc=[]\n",
    "            episodes=5000\n",
    "            for episode in tqdm(range(1,episodes+1),desc=\"Q Learning\"):\n",
    "                #state=env.reset()[0]\n",
    "                x_episodes.append(episode)\n",
    "                reward_after_each_episode_ql.append(gen_episode_ql(q_table_ql))\n",
    "                epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "            results.append((alpha,gamma,epsilon,reward_after_each_episode_ql))\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.grid(True)\n",
    "for alpha,gamma,epsilon,reward_after_each_episode in results:\n",
    "#print(reward_after_each_episode_ql,reward_after_each_episode_mc)\n",
    "    #plt.figure(figsize=(10,6))\n",
    "    #plt.grid(True)\n",
    "    plt.plot(x_episodes,reward_after_each_episode,label=f'α={alpha}, γ={gamma}, ε={epsilon}')\n",
    "plt.title('Q-Learning Rewards')\n",
    "plt.xlabel('rewards')\n",
    "plt.ylabel('episodes')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    #plt.plot(x_episodes,reward_after_each_episode_mc)\n",
    "    #plt.legend\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dbbab-eccc-4fb8-b4fe-10781b5a5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##trial-so even that updation happens in function for a global variable tho it may not return it \n",
    "p=[]\n",
    "h=1\n",
    "def gen(h):\n",
    "    for i in range(5):\n",
    "        h=h+1\n",
    "        p.append(h)\n",
    "    return(h)\n",
    "print(gen(h))\n",
    "print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36991381-10ef-414a-a833-52627822f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"`np.bool8` is a deprecated alias for `np.bool_`\")\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1          # Learning rate\n",
    "gamma = 0.99         # Discount factor\n",
    "epsilon = 0.1        # Exploration rate\n",
    "epsilon_decay = 0.99 # Decay rate for epsilon\n",
    "min_epsilon = 0.01   # Minimum epsilon value\n",
    "episodes = 1000      # Number of episodes for training\n",
    "num_states = 500     # Number of states (adjust as needed)\n",
    "\n",
    "# Initialize Q-table as a dictionary of NumPy arrays\n",
    "q_table = {i: np.zeros(env.action_space.n) for i in range(num_states)}\n",
    "\n",
    "# Lists to store results for plotting\n",
    "x_episodes = []\n",
    "reward_after_each_episode = []\n",
    "\n",
    "# Function to choose action based on epsilon-greedy policy\n",
    "def take_action(state, q_table, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()  # Explore: choose random action\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploit: choose action with highest Q-value\n",
    "\n",
    "# Training loop for Monte Carlo\n",
    "def train_monte_carlo(alpha, gamma, epsilon):\n",
    "    q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    cumulative_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_data = []\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = take_action(state, q_table, epsilon)\n",
    "            next_state, reward, done, _ ,_= env.step(action)\n",
    "            episode_data.append((state, action, reward))\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        # Monte Carlo update\n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode_data):\n",
    "            G = gamma * G + reward\n",
    "            q_table[state][action] += alpha * (G - q_table[state][action])\n",
    "\n",
    "        cumulative_rewards.append(total_reward)\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "# Training loop for TD (Q-learning)\n",
    "def train_td(alpha, gamma, epsilon):\n",
    "    q_table = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    cumulative_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = take_action(state, q_table, epsilon)\n",
    "            next_state, reward, done, _ ,_= env.step(action)\n",
    "            old_q_value = q_table[state][action]\n",
    "            max_next_q_value = np.max(q_table[next_state]) if not done else 0\n",
    "            q_table[state][action] = old_q_value + alpha * (reward + gamma * max_next_q_value - old_q_value)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        cumulative_rewards.append(total_reward)\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "# Experiment with different parameter values\n",
    "for alpha in [0.1]:\n",
    "    for gamma in [0.99]:\n",
    "        for epsilon in [0.1]:\n",
    "            mc_rewards = train_monte_carlo(alpha, gamma, epsilon)\n",
    "            td_rewards = train_td(alpha, gamma, epsilon)\n",
    "\n",
    "            # Plotting cumulative rewards for comparison\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(episodes), mc_rewards, label='Monte Carlo')\n",
    "            plt.plot(range(episodes), td_rewards, label='TD')\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.ylabel('Cumulative Reward')\n",
    "            plt.title(f'Alpha: {alpha}, Gamma: {gamma}, Epsilon: {epsilon}')\n",
    "            plt.legend()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e01f4-593d-4188-aa92-91872c1b5932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q Learning:   0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\Shraddha\\.anaconda\\anaconda-navigator\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "Q Learning: 100%|██████████| 500/500 [00:01<00:00, 306.59it/s]\n",
      "Monte Carlo:   0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"`np.bool8` is a deprecated alias for `np.bool_`\")\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize environment\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "epsilon_decay = 0.99\n",
    "min_epsilon = 0.01\n",
    "episodes = 500  # Number of episodes for training\n",
    "\n",
    "# Initialize Q-tables\n",
    "q_table_ql = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "q_table_mc = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Lists to store results for plotting\n",
    "x_episodes = []\n",
    "reward_after_each_episode_ql = []\n",
    "reward_after_each_episode_mc = []\n",
    "\n",
    "# Function to choose action based on epsilon-greedy policy\n",
    "def take_action(state, q_table, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()  # Explore: choose random action\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploit: choose action with highest Q-value\n",
    "\n",
    "# Q-Learning episode generator\n",
    "def gen_episode_ql(q_table):\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = take_action(state, q_table, epsilon)\n",
    "        next_state, reward, done, _ ,_= env.step(action)\n",
    "        old_q_value = q_table[state][action]\n",
    "        max_next_q_value = np.max(q_table[next_state]) if not done else 0\n",
    "        q_table[state][action] = old_q_value + alpha * (reward + gamma * max_next_q_value - old_q_value)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Monte Carlo episode generator\n",
    "def gen_episode_mc(q_table):\n",
    "    state = env.reset()[0]\n",
    "    episode_data = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = take_action(state, q_table, epsilon)\n",
    "        next_state, reward, done, _,_ = env.step(action)\n",
    "        episode_data.append((state, action, reward))\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    return episode_data, total_reward\n",
    "\n",
    "# Training loop for Q-Learning\n",
    "for episode in tqdm(range(1, episodes + 1), desc=\"Q Learning\"):\n",
    "    x_episodes.append(episode)\n",
    "    reward_after_each_episode_ql.append(gen_episode_ql(q_table_ql))\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Reset epsilon for Monte Carlo\n",
    "epsilon = 0.1\n",
    "\n",
    "# Training loop for Monte Carlo\n",
    "for episode in tqdm(range(1, episodes + 1), desc=\"Monte Carlo\"):\n",
    "    episode_data, total_reward = gen_episode_mc(q_table_mc)\n",
    "    reward_after_each_episode_mc.append(total_reward)\n",
    "    G = 0\n",
    "    for state, action, reward in reversed(episode_data):\n",
    "        G = gamma * G + reward\n",
    "        q_table_mc[state][action] += alpha * (G - q_table_mc[state][action])\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.plot(x_episodes, reward_after_each_episode_ql)\n",
    "plt.plot(x_episodes, reward_after_each_episode_mc)\n",
    "plt.legend(['Q-Learning', 'Monte Carlo'])\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward per Episode')\n",
    "plt.title('Q-Learning vs Monte Carlo')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f77f973-e672-49d5-ae0a-6321b7a2d73d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
